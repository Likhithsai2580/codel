package providers

import (
	"context"
	"database/sql"
	"encoding/json"
	"fmt"
	"log"

	"github.com/semanser/ai-coder/assets"
	"github.com/semanser/ai-coder/config"
	"github.com/semanser/ai-coder/database"
	"github.com/semanser/ai-coder/templates"

	"github.com/invopop/jsonschema"
	"github.com/tmc/langchaingo/llms"
	"github.com/tmc/langchaingo/schema"
)

type ProviderType string

const (
	ProviderOpenAI ProviderType = "openai"
)

type Provider interface {
	New() Provider
	Name() ProviderType
	Summary(query string, n int) (string, error)
	DockerImageName(task string) (string, error)
	NextTask(args NextTaskOptions) *database.Task
}

type NextTaskOptions struct {
	Tasks       []database.Task
	DockerImage string
}

func ProviderFactory(provider ProviderType) (Provider, error) {
	switch provider {
	case ProviderOpenAI:
		return OpenAIProvider{}.New(), nil
	default:
		return nil, fmt.Errorf("unknown provider: %s", provider)
	}
}

func Summary(llm llms.Model, model string, query string, n int) (string, error) {
	prompt, err := templates.Render(assets.PromptTemplates, "prompts/summary.tmpl", map[string]any{
		"Text": query,
		"N":    n,
	})
	if err != nil {
		return "", err
	}

	response, err := llms.GenerateFromSinglePrompt(
		context.Background(),
		llm,
		prompt,
		llms.WithTemperature(0.0),
		// Use a simpler model for this task
		llms.WithModel(model),
		llms.WithTopP(0.2),
		llms.WithN(1),
	)

	return response, err
}

func DockerImageName(llm llms.Model, model string, task string) (string, error) {
	prompt, err := templates.Render(assets.PromptTemplates, "prompts/docker.tmpl", map[string]any{
		"Task": task,
	})
	if err != nil {
		return "", err
	}

	response, err := llms.GenerateFromSinglePrompt(
		context.Background(),
		llm,
		prompt,
		llms.WithTemperature(0.0),
		llms.WithModel(model),
		llms.WithTopP(0.2),
		llms.WithN(1),
	)

	return response, err
}

func NextTask(args NextTaskOptions, llm llms.Model) *database.Task {
	log.Println("Getting next task")

	prompt, err := templates.Render(assets.PromptTemplates, "prompts/agent.tmpl", args)

	// TODO In case of lots of tasks, we should try to get a summary using gpt-3.5
	if len(prompt) > 30000 {
		log.Println("Prompt too long, asking user")
		return defaultAskTask("My prompt is too long and I can't process it")
	}

	if err != nil {
		log.Println("Failed to render prompt, asking user, %w", err)
		return defaultAskTask("There was an error getting the next task")
	}

	tools := []llms.Tool{
		{
			Type: "function",
			Function: &llms.FunctionDefinition{
				Name:        "terminal",
				Description: "Calls a terminal command",
				Parameters:  jsonschema.Reflect(&TerminalArgs{}).Definitions["TerminalArgs"],
			},
		},
		{
			Type: "function",
			Function: &llms.FunctionDefinition{
				Name:        "browser",
				Description: "Opens a browser to look for additional information",
				Parameters:  jsonschema.Reflect(&BrowserArgs{}).Definitions["BrowserArgs"],
			},
		},
		{
			Type: "function",
			Function: &llms.FunctionDefinition{
				Name:        "code",
				Description: "Modifies or reads code files",
				Parameters:  jsonschema.Reflect(&CodeArgs{}).Definitions["CodeArgs"],
			},
		},
		{
			Type: "function",
			Function: &llms.FunctionDefinition{
				Name:        "ask",
				Description: "Sends a question to the user for additional information",
				Parameters:  jsonschema.Reflect(&AskArgs{}).Definitions["AskArgs"],
			},
		},
		{
			Type: "function",
			Function: &llms.FunctionDefinition{
				Name:        "done",
				Description: "Mark the whole task as done. Should be called at the very end when everything is completed",
				Parameters:  jsonschema.Reflect(&DoneArgs{}).Definitions["DoneArgs"],
			},
		},
	}

	var messages []llms.MessageContent

	messages = append(messages, llms.MessageContent{
		Role: schema.ChatMessageTypeSystem,
		Parts: []llms.ContentPart{
			llms.TextPart(prompt),
		},
	})

	for _, task := range args.Tasks {
		if task.Type.String == "input" {
			messages = append(messages, llms.MessageContent{
				Role: schema.ChatMessageTypeHuman,
				Parts: []llms.ContentPart{
					llms.TextPart(prompt),
				},
			})
		}

		if task.ToolCallID.String != "" {
			messages = append(messages, llms.MessageContent{
				Role: schema.ChatMessageTypeAI,
				Parts: []llms.ContentPart{
					llms.ToolCall{
						ID: task.ToolCallID.String,
						FunctionCall: &schema.FunctionCall{
							Name:      task.Type.String,
							Arguments: task.Args.String,
						},
						Type: "function",
					},
				},
			})

			messages = append(messages, llms.MessageContent{
				Role: schema.ChatMessageTypeTool,
				Parts: []llms.ContentPart{
					llms.ToolCallResponse{
						ToolCallID: task.ToolCallID.String,
						Name:       task.Type.String,
						Content:    task.Results.String,
					},
				},
			})
		}

		// This Ask was generated by the agent itself in case of some error (not the OpenAI)
		if task.Type.String == "ask" && task.ToolCallID.String == "" {
			messages = append(messages, llms.MessageContent{
				Role: schema.ChatMessageTypeAI,
				Parts: []llms.ContentPart{
					llms.TextPart(task.Message.String),
				},
			})
		}
	}

	resp, err := llm.GenerateContent(
		context.Background(),
		messages,
		llms.WithTemperature(0.0),
		llms.WithModel(config.Config.OpenAIModel),
		llms.WithTopP(0.2),
		llms.WithN(1),
		llms.WithTools(tools),
	)

	if err != nil {
		log.Printf("Failed to get response from OpenAI %v", err)
		return defaultAskTask("There was an error getting the next task")
	}

	choices := resp.Choices

	if len(choices) == 0 {
		log.Println("No choices found, asking user")
		return defaultAskTask("Looks like I couldn't find a task to run")
	}

	toolCalls := choices[0].ToolCalls

	if len(toolCalls) == 0 {
		log.Println("No tool calls found, asking user")
		return defaultAskTask("I couln't find a task to run")
	}

	tool := toolCalls[0]

	if tool.FunctionCall.Name == "" {
		log.Println("No tool found, asking user")
		return defaultAskTask("The next task is empty, I don't know what to do next")
	}

	task := database.Task{
		Type: database.StringToNullString(tool.FunctionCall.Name),
	}

	switch tool.FunctionCall.Name {
	case "terminal":
		params, err := extractArgs(tool.FunctionCall.Arguments, &TerminalArgs{})
		if err != nil {
			log.Printf("Failed to extract terminal args, asking user: %v", err)
			return defaultAskTask("There was an error running the terminal command")
		}
		args, err := json.Marshal(params)
		if err != nil {
			log.Printf("Failed to marshal terminal args, asking user: %v", err)
			return defaultAskTask("There was an error running the terminal command")
		}
		task.Args = database.StringToNullString(string(args))

		// Sometimes the model returns an empty string for the message
		msg := string(params.Message)
		if msg == "" {
			msg = params.Input
		}

		task.Message = database.StringToNullString(msg)
		task.Status = database.StringToNullString("in_progress")

	case "browser":
		params, err := extractArgs(tool.FunctionCall.Arguments, &BrowserArgs{})
		if err != nil {
			log.Printf("Failed to extract browser args, asking user: %v", err)
			return defaultAskTask("There was an error opening the browser")
		}
		args, err := json.Marshal(params)
		if err != nil {
			log.Printf("Failed to marshal browser args, asking user: %v", err)
			return defaultAskTask("There was an error opening the browser")
		}
		task.Args = database.StringToNullString(string(args))
		task.Message = database.StringToNullString(string(params.Message))
	case "code":
		params, err := extractArgs(tool.FunctionCall.Arguments, &CodeArgs{})
		if err != nil {
			log.Printf("Failed to extract code args, asking user: %v", err)
			return defaultAskTask("There was an error reading or updating the file")
		}
		args, err := json.Marshal(params)
		if err != nil {
			log.Printf("Failed to marshal code args, asking user: %v", err)
			return defaultAskTask("There was an error reading or updating the file")
		}
		task.Args = database.StringToNullString(string(args))
		task.Message = database.StringToNullString(string(params.Message))
	case "ask":
		params, err := extractArgs(tool.FunctionCall.Arguments, &AskArgs{})
		if err != nil {
			log.Printf("Failed to extract ask args, asking user: %v", err)
			return defaultAskTask("There was an error asking the user for additional information")
		}
		args, err := json.Marshal(params)
		if err != nil {
			log.Printf("Failed to marshal ask args, asking user: %v", err)
			return defaultAskTask("There was an error asking the user for additional information")
		}
		task.Args = database.StringToNullString(string(args))
		task.Message = database.StringToNullString(string(params.Message))
	case "done":
		params, err := extractArgs(tool.FunctionCall.Arguments, &DoneArgs{})
		if err != nil {
			log.Printf("Failed to extract done args, asking user: %v", err)
			return defaultAskTask("There was an error marking the task as done")
		}
		args, err := json.Marshal(params)
		if err != nil {
			return defaultAskTask("There was an error marking the task as done")
		}
		task.Args = database.StringToNullString(string(args))
		task.Message = database.StringToNullString(string(params.Message))
	}

	task.ToolCallID = database.StringToNullString(tool.ID)

	return &task
}

func defaultAskTask(message string) *database.Task {
	task := database.Task{
		Type: database.StringToNullString("ask"),
	}

	task.Args = database.StringToNullString("{}")
	task.Message = sql.NullString{
		String: fmt.Sprintf("%s. What should I do next?", message),
		Valid:  true,
	}

	return &task
}

func extractArgs[T any](openAIargs string, args *T) (*T, error) {
	err := json.Unmarshal([]byte(openAIargs), args)
	if err != nil {
		return nil, fmt.Errorf("failed to unmarshal args: %v", err)
	}
	return args, nil
}
